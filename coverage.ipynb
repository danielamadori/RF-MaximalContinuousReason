{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbcd7fb3",
   "metadata": {},
   "source": [
    "# Result comparison with [AXp](https://arxiv.org/abs/2105.10278)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3258c21",
   "metadata": {},
   "source": [
    "## [AXp](https://arxiv.org/abs/2105.10278) RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c617b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASELINE_ROOT = Path('baseline/experiments')\n",
    "def extract_baseline_metrics(json_path):\n",
    "    with json_path.open('r', encoding='utf-8') as handle:\n",
    "        data = json.load(handle)\n",
    "\n",
    "    dataset_name = data.get('bench_name') or json_path.parent.name\n",
    "    experiments = data.get('experiments', [])\n",
    "    if not experiments:\n",
    "        return None\n",
    "\n",
    "    def metric_values(key):\n",
    "        values = []\n",
    "        for exp in experiments:\n",
    "            explanations = exp.get('explanations', {})\n",
    "            value = explanations.get(key)\n",
    "            if value is not None:\n",
    "                values.append(value)\n",
    "        return values\n",
    "\n",
    "    avg_times = metric_values('avg_explanation_time')\n",
    "    min_times = metric_values('min_explanation_time')\n",
    "    max_times = metric_values('max_explanation_time')\n",
    "    avg_lengths = metric_values('avg_explanation_length')\n",
    "    min_lengths = metric_values('min_explanation_length')\n",
    "    max_lengths = metric_values('max_explanation_length')\n",
    "    full_explanations = metric_values('full_explanations')[0]\n",
    "    sum_explanations_time = metric_values('explanation_times')[0]\n",
    "    len_feature = max((len(v) for v in full_explanations))\n",
    "    coverage_avg = []\n",
    "    for explanations in full_explanations:\n",
    "        coverage = (len_feature - len(explanations))/len_feature *100\n",
    "        coverage_avg.append(coverage)\n",
    "    return {\n",
    "        'dataset': dataset_name,\n",
    "        'avg_explanation_time': sum(avg_times) / len(avg_times) if avg_times else None,\n",
    "        'min_explanation_time': min(min_times) if min_times else None,\n",
    "        'max_explanation_time': max(max_times) if max_times else None,\n",
    "        'avg_explanation_length': int(sum(avg_lengths) / len(avg_lengths)) if avg_lengths else None,\n",
    "        'min_explanation_length': min(min_lengths) if min_lengths else None,\n",
    "        'max_explanation_length': max(max_lengths) if max_lengths else None,\n",
    "        # 'full_explanations': full_explanations if full_explanations else None,\n",
    "        'coverage_avg': sum(coverage_avg) / len(coverage_avg) if coverage_avg else None,\n",
    "        'sum_explanations_time': sum(sum_explanations_time) if sum_explanations_time else None,\n",
    "    }\n",
    "\n",
    "baseline_files = sorted(BASELINE_ROOT.glob('*/*_results.json'))\n",
    "if not baseline_files:\n",
    "    raise FileNotFoundError('No baseline results found under baseline/experiments')\n",
    "\n",
    "records = []\n",
    "for path in baseline_files:\n",
    "    record = extract_baseline_metrics(path)\n",
    "    if record:\n",
    "        records.append(record)\n",
    "\n",
    "baseline_df = (\n",
    "    pd.DataFrame.from_records(records)\n",
    "    .sort_values('dataset')\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "baseline_df['time_seconds'] = baseline_df['sum_explanations_time']\n",
    "baseline_df.sort_values('dataset').to_csv('baseline_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff4872",
   "metadata": {},
   "source": [
    "## Our Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1tcvq2d7cvm",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:50:59.180215Z",
     "start_time": "2026-01-26T19:50:59.164862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>axp_cov</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ann-thyroid</td>\n",
       "      <td>86.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>appendicitis</td>\n",
       "      <td>42.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>banknote</td>\n",
       "      <td>29.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>biodegradation</td>\n",
       "      <td>45.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ecoli</td>\n",
       "      <td>21.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>glass2</td>\n",
       "      <td>24.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>heart-c</td>\n",
       "      <td>33.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ionosphere</td>\n",
       "      <td>17.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>iris</td>\n",
       "      <td>13.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>karhunen</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>letter</td>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>magic</td>\n",
       "      <td>23.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mofn-3-7-10</td>\n",
       "      <td>67.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>new-thyroid</td>\n",
       "      <td>29.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pendigits</td>\n",
       "      <td>2.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phoneme</td>\n",
       "      <td>31.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ring</td>\n",
       "      <td>39.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>segmentation</td>\n",
       "      <td>11.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>shuttle</td>\n",
       "      <td>61.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sonar</td>\n",
       "      <td>18.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>spambase</td>\n",
       "      <td>21.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>spectf</td>\n",
       "      <td>49.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>texture</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>threeOf9</td>\n",
       "      <td>50.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>twonorm</td>\n",
       "      <td>23.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>vowel</td>\n",
       "      <td>4.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>waveform-21</td>\n",
       "      <td>22.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>waveform-40</td>\n",
       "      <td>16.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>wdbc</td>\n",
       "      <td>41.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>wine-recog</td>\n",
       "      <td>18.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>wpbc</td>\n",
       "      <td>38.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>xd6</td>\n",
       "      <td>53.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dataset  axp_cov\n",
       "0      ann-thyroid    86.54\n",
       "1     appendicitis    42.52\n",
       "2         banknote    29.71\n",
       "3   biodegradation    45.82\n",
       "4            ecoli    21.72\n",
       "5           glass2    24.75\n",
       "6          heart-c    33.17\n",
       "7       ionosphere    17.97\n",
       "8             iris    13.75\n",
       "9         karhunen     3.51\n",
       "10          letter     1.38\n",
       "11           magic    23.75\n",
       "12     mofn-3-7-10    67.82\n",
       "13     new-thyroid    29.30\n",
       "14       pendigits     2.81\n",
       "15         phoneme    31.50\n",
       "16            ring    39.57\n",
       "17    segmentation    11.38\n",
       "18         shuttle    61.28\n",
       "19           sonar    18.97\n",
       "20        spambase    21.72\n",
       "21          spectf    49.92\n",
       "22         texture     4.40\n",
       "23        threeOf9    50.12\n",
       "24         twonorm    23.80\n",
       "25           vowel     4.35\n",
       "26     waveform-21    22.59\n",
       "27     waveform-40    16.02\n",
       "28            wdbc    41.23\n",
       "29      wine-recog    18.70\n",
       "30            wpbc    38.38\n",
       "31             xd6    53.50"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'dataset': [\n",
    "        'ann-thyroid', 'appendicitis', 'banknote', 'biodegradation', 'ecoli',\n",
    "        'glass2', 'heart-c', 'ionosphere', 'iris', 'karhunen', 'letter',\n",
    "        'magic', 'mofn-3-7-10', 'new-thyroid', 'pendigits', 'phoneme', 'ring',\n",
    "        'segmentation', 'shuttle', 'sonar', 'spambase', 'spectf', 'texture',\n",
    "        'threeOf9', 'twonorm', 'vowel', 'waveform-21', 'waveform-40', 'wdbc',\n",
    "        'wine-recog', 'wpbc', 'xd6'\n",
    "    ],\n",
    "    'axp_cov': [\n",
    "        86.54, 42.52, 29.71, 45.82, 21.72, 24.75, 33.17, 17.97, 13.75, 3.51,\n",
    "        1.38, 23.75, 67.82, 29.30, 2.81, 31.50, 39.57, 11.38, 61.28, 18.97,\n",
    "        21.72, 49.92, 4.40, 50.12, 23.80, 4.35, 22.59, 16.02, 41.23, 18.70,\n",
    "        38.38, 53.50\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_axp = pd.DataFrame(data)\n",
    "df_axp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "xojrmh0k8w",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:51:03.538630Z",
     "start_time": "2026-01-26T19:51:00.092347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available worker configurations: []\n",
      "\n",
      "No files found for workers_32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_coverage_data(n_workers):\n",
    "    \"\"\"\n",
    "    Load coverage data for a given number of workers.\n",
    "    Dynamically finds which datasets have that worker configuration.\n",
    "    \n",
    "    Returns: DataFrame with all binary key data, or empty DataFrame if none found\n",
    "    \"\"\"\n",
    "    base_path = Path(\"results/checkpoints_scalability\")\n",
    "    pattern = f\"*/workers_{n_workers}/class_*/all_features/redis_dump_readable.json\"\n",
    "    json_files = list(base_path.glob(pattern))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No files found for workers_{n_workers}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Find unique datasets\n",
    "    datasets = sorted(set(f.parts[2] for f in json_files))\n",
    "    print(f\"Found {len(json_files)} files for workers_{n_workers}\")\n",
    "    print(f\"Datasets: {datasets}\")\n",
    "    \n",
    "    rows = []\n",
    "    all_binary_keys = set()\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        parts = json_file.parts\n",
    "        dataset_name = parts[2]\n",
    "        class_label = parts[4].replace(\"class_\", \"\")\n",
    "        \n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        db_dump = data.get(\"0\", {})\n",
    "        \n",
    "        for key, values in db_dump.items():\n",
    "            # Filter only binary keys (contain only 0s and 1s)\n",
    "            if set(key).issubset({'0', '1'}) and len(key) > 0:\n",
    "                all_binary_keys.add(key)\n",
    "                row = {\n",
    "                    'dataset_name': dataset_name,\n",
    "                    'class_label': class_label,\n",
    "                    'binary_key': key,\n",
    "                    'worker': values.get('worker'),\n",
    "                    'timeout_occurred': values.get('timeout_occurred'),\n",
    "                    'coverage': float(values.get('coverage', 0)),\n",
    "                    'vts': values.get('vts'),\n",
    "                    'vte': values.get('vte'),\n",
    "                    'coverage_t': values.get('coverage_t'),\n",
    "                    'globally_dominated': values.get('globally_dominated')\n",
    "                }\n",
    "                rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Sort binary keys lexicographically and create ID mapping\n",
    "    sorted_keys = sorted(all_binary_keys)\n",
    "    key_to_id = {k: i for i, k in enumerate(sorted_keys)}\n",
    "    \n",
    "    df['binary_key_id'] = df['binary_key'].map(key_to_id)\n",
    "    \n",
    "    df = df[['dataset_name', 'class_label', 'binary_key_id', 'binary_key', \n",
    "             'worker', 'timeout_occurred', 'coverage', 'vts', 'vte', \n",
    "             'coverage_t', 'globally_dominated']]\n",
    "    \n",
    "    df = df.sort_values(['dataset_name', 'class_label', 'binary_key_id']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Unique binary keys: {len(all_binary_keys)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_available_worker_configs():\n",
    "    \"\"\"Find all available worker configurations.\"\"\"\n",
    "    base_path = Path(\"results/checkpoints_scalability\")\n",
    "    worker_dirs = base_path.glob(\"*/workers_*\")\n",
    "    configs = set()\n",
    "    for d in worker_dirs:\n",
    "        n = d.name.replace(\"workers_\", \"\")\n",
    "        if n.isdigit():\n",
    "            configs.add(int(n))\n",
    "    return sorted(configs)\n",
    "\n",
    "\n",
    "# Show available configurations\n",
    "available = get_available_worker_configs()\n",
    "print(f\"Available worker configurations: {available}\\n\")\n",
    "\n",
    "# Load data for workers_32\n",
    "df_32 = load_coverage_data(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "egk9mce4rnb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:51:03.553686Z",
     "start_time": "2026-01-26T19:51:03.539350Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data for workers_16\n",
    "# df_16 = load_coverage_data(16)\n",
    "# df_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "of50mx3hf1q",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:51:03.582329Z",
     "start_time": "2026-01-26T19:51:03.557604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data available for workers_32 (results/checkpoints_scalability not found)\n"
     ]
    }
   ],
   "source": [
    "# Average coverage per dataset (as percentage) - workers_32\n",
    "if df_32.empty:\n",
    "    print(\"No data available for workers_32 (results/checkpoints_scalability not found)\")\n",
    "    avg_cov_32 = pd.DataFrame(columns=['dataset', 'our_cov'])\n",
    "else:\n",
    "    avg_cov_32 = df_32.groupby('dataset_name')['coverage'].mean() * 100\n",
    "    avg_cov_32 = avg_cov_32.reset_index()\n",
    "    avg_cov_32.columns = ['dataset', 'our_cov']\n",
    "    print(\"Workers 32:\")\n",
    "    display(avg_cov_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cxrwn45q3ta",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:51:04.564409Z",
     "start_time": "2026-01-26T19:51:04.545387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>axp_cov</th>\n",
       "      <th>our_cov</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ann-thyroid</td>\n",
       "      <td>86.54</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>appendicitis</td>\n",
       "      <td>42.52</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>banknote</td>\n",
       "      <td>29.71</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>biodegradation</td>\n",
       "      <td>45.82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ecoli</td>\n",
       "      <td>21.72</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>glass2</td>\n",
       "      <td>24.75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>heart-c</td>\n",
       "      <td>33.17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ionosphere</td>\n",
       "      <td>17.97</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>iris</td>\n",
       "      <td>13.75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>karhunen</td>\n",
       "      <td>3.51</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>letter</td>\n",
       "      <td>1.38</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>magic</td>\n",
       "      <td>23.75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mofn-3-7-10</td>\n",
       "      <td>67.82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>new-thyroid</td>\n",
       "      <td>29.30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pendigits</td>\n",
       "      <td>2.81</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phoneme</td>\n",
       "      <td>31.50</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ring</td>\n",
       "      <td>39.57</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>segmentation</td>\n",
       "      <td>11.38</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>shuttle</td>\n",
       "      <td>61.28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sonar</td>\n",
       "      <td>18.97</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>spambase</td>\n",
       "      <td>21.72</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>spectf</td>\n",
       "      <td>49.92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>texture</td>\n",
       "      <td>4.40</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>threeOf9</td>\n",
       "      <td>50.12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>twonorm</td>\n",
       "      <td>23.80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>vowel</td>\n",
       "      <td>4.35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>waveform-21</td>\n",
       "      <td>22.59</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>waveform-40</td>\n",
       "      <td>16.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>wdbc</td>\n",
       "      <td>41.23</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>wine-recog</td>\n",
       "      <td>18.70</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>wpbc</td>\n",
       "      <td>38.38</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>xd6</td>\n",
       "      <td>53.50</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dataset  axp_cov our_cov\n",
       "0      ann-thyroid    86.54     NaN\n",
       "1     appendicitis    42.52     NaN\n",
       "2         banknote    29.71     NaN\n",
       "3   biodegradation    45.82     NaN\n",
       "4            ecoli    21.72     NaN\n",
       "5           glass2    24.75     NaN\n",
       "6          heart-c    33.17     NaN\n",
       "7       ionosphere    17.97     NaN\n",
       "8             iris    13.75     NaN\n",
       "9         karhunen     3.51     NaN\n",
       "10          letter     1.38     NaN\n",
       "11           magic    23.75     NaN\n",
       "12     mofn-3-7-10    67.82     NaN\n",
       "13     new-thyroid    29.30     NaN\n",
       "14       pendigits     2.81     NaN\n",
       "15         phoneme    31.50     NaN\n",
       "16            ring    39.57     NaN\n",
       "17    segmentation    11.38     NaN\n",
       "18         shuttle    61.28     NaN\n",
       "19           sonar    18.97     NaN\n",
       "20        spambase    21.72     NaN\n",
       "21          spectf    49.92     NaN\n",
       "22         texture     4.40     NaN\n",
       "23        threeOf9    50.12     NaN\n",
       "24         twonorm    23.80     NaN\n",
       "25           vowel     4.35     NaN\n",
       "26     waveform-21    22.59     NaN\n",
       "27     waveform-40    16.02     NaN\n",
       "28            wdbc    41.23     NaN\n",
       "29      wine-recog    18.70     NaN\n",
       "30            wpbc    38.38     NaN\n",
       "31             xd6    53.50     NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge all coverage data\n",
    "df_comparison = df_axp.merge(avg_cov_32, on='dataset', how='outer')\n",
    "df_comparison = df_comparison.sort_values('dataset').reset_index(drop=True)\n",
    "df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "h6zda9bb49k",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:51:05.412798Z",
     "start_time": "2026-01-26T19:51:05.385896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No paired data available — run experiments to populate results/checkpoints_scalability\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Filter rows where both values are not NaN\n",
    "df_both = df_comparison.dropna(subset=['axp_cov', 'our_cov'])\n",
    "\n",
    "if df_both.empty:\n",
    "    print(\"No paired data available — run experiments to populate results/checkpoints_scalability\")\n",
    "    t_stat, p_value = float('nan'), float('nan')\n",
    "else:\n",
    "    # Paired t-test (same datasets, two methods)\n",
    "    t_stat, p_value = stats.ttest_rel(df_both['axp_cov'], df_both['our_cov'])\n",
    "\n",
    "    print(f\"Datasets with both values: {len(df_both)}\")\n",
    "    print(f\"Mean axp_cov: {df_both['axp_cov'].mean():.2f}\")\n",
    "    print(f\"Mean our_cov: {df_both['our_cov'].mean():.2f}\")\n",
    "    print(f\"\\nPaired t-test:\")\n",
    "    print(f\"t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"p-value: {p_value:.4e}\")\n",
    "    display(df_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mindcqo844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:51:06.303946Z",
     "start_time": "2026-01-26T19:51:06.285854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No paired data available for t-table — skipped\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from scipy.stats import t\n",
    "\n",
    "if df_both.empty or math.isnan(t_stat):\n",
    "    print(\"No paired data available for t-table — skipped\")\n",
    "else:\n",
    "    df_degrees = len(df_both) - 1  # degrees of freedom\n",
    "\n",
    "    # Critical t-values for common confidence levels (two-tailed)\n",
    "    alpha_levels = [0.10, 0.05, 0.01, 0.001]\n",
    "\n",
    "    print(f\"t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"Degrees of freedom: {df_degrees}\")\n",
    "    print(f\"\\nT-table critical values (two-tailed):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'α':<10} {'Confidence':<15} {'t_critical':<12} {'Significant'}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for alpha in alpha_levels:\n",
    "        confidence = (1 - alpha) * 100\n",
    "        t_critical = t.ppf(1 - alpha/2, df_degrees)\n",
    "        significant = abs(t_stat) > t_critical\n",
    "        symbol = \"✓\" if significant else \"✗\"\n",
    "        print(f\"{alpha:<10} {confidence:.1f}%{'':<9} {t_critical:<12.4f} {symbol}\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"\\n|t| = {abs(t_stat):.4f}\")\n",
    "    print(f\"p-value = {p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c2376",
   "metadata": {},
   "source": [
    "# Scalability Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d216e6eb9971ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:51:18.308891Z",
     "start_time": "2026-01-26T19:51:14.417826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found for workers_16\n",
      "No files found for workers_32\n",
      "No scalability data available — run experiments to populate results/checkpoints_scalability\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TIMEOUT_SECONDS = 6 * 60  # 6 minutes\n",
    "\n",
    "df_16 = load_coverage_data(16)\n",
    "df_32 = load_coverage_data(32)\n",
    "\n",
    "scalability = pd.DataFrame()  # default empty\n",
    "\n",
    "if df_16.empty or df_32.empty:\n",
    "    print(\"No scalability data available — run experiments to populate results/checkpoints_scalability\")\n",
    "else:\n",
    "    common_datasets = set(df_16['dataset_name'].unique()) & set(df_32['dataset_name'].unique())\n",
    "    print(f\"\\nCommon datasets for comparison: {common_datasets}\")\n",
    "\n",
    "    df_16_common = df_16[df_16['dataset_name'].isin(common_datasets)].copy()\n",
    "    df_32_common = df_32[df_32['dataset_name'].isin(common_datasets)].copy()\n",
    "\n",
    "    df_16_common['sample_time'] = (df_16_common['vte'].astype(float) - df_16_common['vts'].astype(float)).clip(upper=TIMEOUT_SECONDS)\n",
    "    df_32_common['sample_time'] = (df_32_common['vte'].astype(float) - df_32_common['vts'].astype(float)).clip(upper=TIMEOUT_SECONDS)\n",
    "\n",
    "    def get_class_time(df):\n",
    "        worker_times = df.groupby(['dataset_name', 'class_label', 'worker'])['sample_time'].sum().reset_index()\n",
    "        class_times = worker_times.groupby(['dataset_name', 'class_label'])['sample_time'].max().reset_index()\n",
    "        class_times.columns = ['dataset_name', 'class_label', 'class_time']\n",
    "        return class_times\n",
    "\n",
    "    class_times_16 = get_class_time(df_16_common)\n",
    "    class_times_32 = get_class_time(df_32_common)\n",
    "\n",
    "    class_comparison = class_times_16.merge(class_times_32, on=['dataset_name', 'class_label'], suffixes=('_16', '_32'))\n",
    "    print(f\"Classes compared: {len(class_comparison)}\")\n",
    "\n",
    "    class_comparison['speedup'] = class_comparison['class_time_16'] / class_comparison['class_time_32']\n",
    "\n",
    "    scalability = class_comparison.groupby('dataset_name').agg({\n",
    "        'class_time_16': 'sum',\n",
    "        'class_time_32': 'sum',\n",
    "        'speedup': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    scalability['total_speedup'] = scalability['class_time_16'] / scalability['class_time_32']\n",
    "    scalability['efficiency'] = scalability['total_speedup'] / 2\n",
    "\n",
    "    print(\"\\nPer-class times:\")\n",
    "    display(class_comparison)\n",
    "    print(\"\\nPer-dataset summary:\")\n",
    "    display(scalability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "y4zx2irfv4k",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:51:21.181123Z",
     "start_time": "2026-01-26T19:51:20.374321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No scalability data available — skipped chart\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "if scalability.empty:\n",
    "    print(\"No scalability data available — skipped chart\")\n",
    "else:\n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=(\n",
    "        'Total Class Time per Dataset<br>(timeouts capped at 6 min)',\n",
    "        'Scalability: Speedup',\n",
    "        'Parallel Efficiency'\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(name='16 workers', x=scalability['dataset_name'], y=scalability['class_time_16'] / 60, marker_color='steelblue'), row=1, col=1)\n",
    "    fig.add_trace(go.Bar(name='32 workers', x=scalability['dataset_name'], y=scalability['class_time_32'] / 60, marker_color='coral'), row=1, col=1)\n",
    "    fig.add_trace(go.Bar(name='Parallel Efficiency', x=scalability['dataset_name'], y=scalability['efficiency'] * 100, marker_color='seagreen', showlegend=False), row=1, col=3)\n",
    "    fig.add_hline(y=100, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Ideal (100%)\", row=1, col=3)\n",
    "\n",
    "    fig.update_layout(height=400, barmode='group', title_text='Scalability Analysis: 16 vs 32 Workers')\n",
    "    fig.update_xaxes(tickangle=45)\n",
    "    fig.update_yaxes(title_text='Time (minutes)', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Speedup', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='Efficiency (%)', row=1, col=3)\n",
    "    fig.write_image(\"scalability_analysis.png\")\n",
    "    fig.show()\n",
    "\n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"  Total time 16 workers: {scalability['class_time_16'].sum() / 60:.1f} minutes\")\n",
    "    print(f\"  Total time 32 workers: {scalability['class_time_32'].sum() / 60:.1f} minutes\")\n",
    "    print(f\"  Mean speedup: {scalability['total_speedup'].mean():.2f}x\")\n",
    "    print(f\"  Mean efficiency: {scalability['efficiency'].mean() * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "260993664fd2a400",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:36:16.862812Z",
     "start_time": "2026-01-26T19:36:16.401228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found for workers_16\n"
     ]
    }
   ],
   "source": [
    "df_16 = load_coverage_data(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8ba69e410b232",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:36:17.585389Z",
     "start_time": "2026-01-26T19:36:17.556457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4be1c1e8175273c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T19:18:53.566069Z",
     "start_time": "2026-01-26T19:18:53.551556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb135e13",
   "metadata": {},
   "source": [
    "# Visualization of Maximal continuous reason and the AXp explanation\n",
    "Pick a dataset and class, then randomly select a sample and one of its maximal reasons from the Redis dump. The plot shows the reason interval per feature (vertical line) and the sample value (red point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8da105f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] Redis dump not found at results\\checkpoints\\ecoli\\workers_32\\class_1_sample_all\\redis_dump_readable.json\n",
      "Run experiments first to populate results/checkpoints/\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from redis_helpers.icf import bitmap_to_icf\n",
    "import json\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ── user config ──────────────────────────────────────────────────────────────\n",
    "dataset_name = 'ecoli'\n",
    "class_label  = '1'\n",
    "random_seed  = 17\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "dump_path = Path(\n",
    "    f'results/checkpoints/{dataset_name}/workers_32/class_{class_label}_sample_all/redis_dump_readable.json'\n",
    ")\n",
    "if not dump_path.exists():\n",
    "    print(f\"[SKIP] Redis dump not found at {dump_path}\")\n",
    "    print(\"Run experiments first to populate results/checkpoints/\")\n",
    "    norm_df = None\n",
    "else:\n",
    "    data = json.loads(dump_path.read_text(encoding='utf-8'))\n",
    "    db0 = data['0']\n",
    "    eu = db0['EU']\n",
    "    reasons_db = data.get('2', {})\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    reason_items = [\n",
    "        (bitmap, meta)\n",
    "        for bitmap, meta in reasons_db.items()\n",
    "        if isinstance(meta, dict) and meta.get('sample_key') in db0\n",
    "    ]\n",
    "    if not reason_items:\n",
    "        raise ValueError('No reasons found that map to samples in db0')\n",
    "\n",
    "    reason_bitmap, reason_meta = random.choice(reason_items)\n",
    "    sample_key = reason_meta['sample_key']\n",
    "    sample = db0[sample_key]\n",
    "    icf = bitmap_to_icf(reason_bitmap, eu)\n",
    "\n",
    "    # Build the dataframe with features and intervals\n",
    "    def display_feature_name(name):\n",
    "        if name.startswith('t') and name[1:].isdigit():\n",
    "            return name[1:]\n",
    "        return name\n",
    "\n",
    "    rows = []\n",
    "    for feature in sorted(sample.keys()):\n",
    "        display_feature = display_feature_name(feature)\n",
    "        if feature not in icf:\n",
    "            rows.append({'feature': feature, 'display_feature': display_feature, 'in_reason': False,\n",
    "                         'interval_lower': float('-inf'), 'interval_upper': float('inf'), 'sample_value': sample[feature]})\n",
    "        else:\n",
    "            lower, upper = icf[feature]\n",
    "            rows.append({'feature': feature, 'display_feature': display_feature, 'in_reason': True,\n",
    "                         'interval_lower': lower, 'interval_upper': upper, 'sample_value': sample[feature]})\n",
    "    reason_df = pd.DataFrame(rows)\n",
    "    display(reason_df)\n",
    "\n",
    "    # Normalize per feature to [0, 1] using EU endpoints\n",
    "    def feature_min_max(feature_name):\n",
    "        endpoints = eu.get(feature_name, [])\n",
    "        finite = [v for v in endpoints if math.isfinite(v)]\n",
    "        return (min(finite), max(finite)) if finite else (None, None)\n",
    "\n",
    "    def normalize_value(value, fmin, fmax):\n",
    "        if value == float('-inf'): return 0.0\n",
    "        if value == float('inf'):  return 1.0\n",
    "        if fmin is None or fmax is None or fmax == fmin: return 0.5\n",
    "        return max(0.0, min(1.0, (value - fmin) / (fmax - fmin)))\n",
    "\n",
    "    norm_rows = []\n",
    "    for _, row in reason_df.iterrows():\n",
    "        fmin, fmax = feature_min_max(row['feature'])\n",
    "        norm_lower = normalize_value(row['interval_lower'], fmin, fmax)\n",
    "        norm_upper = normalize_value(row['interval_upper'], fmin, fmax)\n",
    "        if norm_upper < norm_lower: norm_lower, norm_upper = norm_upper, norm_lower\n",
    "        norm_rows.append({\n",
    "            'feature': row['feature'], 'display_feature': row.get('display_feature', row['feature']),\n",
    "            'in_reason': row['in_reason'], 'interval_lower': row['interval_lower'],\n",
    "            'interval_upper': row['interval_upper'], 'sample_value': row['sample_value'],\n",
    "            'norm_lower': norm_lower, 'norm_upper': norm_upper,\n",
    "            'norm_sample': normalize_value(row['sample_value'], fmin, fmax),\n",
    "            'coverage_pct': (norm_upper - norm_lower) * 100.0,\n",
    "            'has_neg_inf': row['interval_lower'] == float('-inf'),\n",
    "            'has_pos_inf': row['interval_upper'] == float('inf'),\n",
    "        })\n",
    "\n",
    "    norm_df = pd.DataFrame(norm_rows)\n",
    "    sample_coverage = norm_df['coverage_pct'].sum() / len(norm_df)\n",
    "    norm_df = norm_df.sort_values('coverage_pct', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Plot\n",
    "    fig_norm = go.Figure()\n",
    "    x_positions = list(range(len(norm_df)))\n",
    "    fig_norm.add_trace(go.Scatter(x=x_positions, y=norm_df['norm_lower'].tolist(), mode='lines', line=dict(width=0), showlegend=False, hoverinfo='skip'))\n",
    "    fig_norm.add_trace(go.Scatter(x=x_positions, y=norm_df['norm_upper'].tolist(), mode='lines', line=dict(width=0), fill='tonexty', fillcolor='rgba(144,238,144,0.35)', name='Reason corridor'))\n",
    "\n",
    "    first_bounds = True\n",
    "    lhw = 0.25\n",
    "    for idx, row in norm_df.iterrows():\n",
    "        if not row['in_reason']: continue\n",
    "        if not row['has_neg_inf']:\n",
    "            fig_norm.add_trace(go.Scatter(x=[idx-lhw, idx+lhw], y=[row['norm_lower']]*2, mode='lines', line=dict(color='green', width=2), name='Reason bounds', showlegend=first_bounds))\n",
    "            first_bounds = False\n",
    "        if not row['has_pos_inf']:\n",
    "            fig_norm.add_trace(go.Scatter(x=[idx-lhw, idx+lhw], y=[row['norm_upper']]*2, mode='lines', line=dict(color='green', width=2), showlegend=False))\n",
    "\n",
    "    in_r_x, in_r_y, ig_x, ig_y = [], [], [], []\n",
    "    for idx, row in norm_df.iterrows():\n",
    "        if row['in_reason']: in_r_x.append(idx); in_r_y.append(row['norm_sample'])\n",
    "        else: ig_x.append(idx); ig_y.append(row['norm_sample'])\n",
    "    if in_r_x: fig_norm.add_trace(go.Scatter(x=in_r_x, y=in_r_y, mode='markers', marker=dict(color='crimson', size=12, line=dict(width=1.5, color='darkred')), name='Sample (in reason)'))\n",
    "    if ig_x:   fig_norm.add_trace(go.Scatter(x=ig_x, y=ig_y, mode='markers', marker=dict(color='lightgray', size=12, line=dict(width=1.5, color='gray')), name='Sample (ignored)'))\n",
    "\n",
    "    for idx, row in norm_df.iterrows():\n",
    "        fig_norm.add_annotation(x=idx, y=1.06, text=f\"{row['coverage_pct']:.1f}%\", showarrow=False, font=dict(size=16, color='black', family='monospace'), xanchor='center')\n",
    "\n",
    "    fig_norm.update_layout(\n",
    "        title=f'Normalized maximal reason ordered by coverage<br><sub>Dataset: {dataset_name}, class: {class_label}, coverage: {sample_coverage:.1f}%</sub>',\n",
    "        xaxis=dict(tickmode='array', tickvals=x_positions, ticktext=norm_df['display_feature'], tickangle=45),\n",
    "        yaxis=dict(range=[-0.05, 1.12]),\n",
    "        height=500, width=1200,\n",
    "    )\n",
    "    try:\n",
    "        fig_norm.write_image(f'maximal_reason_normalized_{dataset_name}_class_{class_label}.pdf')\n",
    "    except Exception as e:\n",
    "        print(f'Save failed: {e}')\n",
    "    fig_norm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22669281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] Baseline visualization skipped — no Redis dump available\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "if norm_df is None:\n",
    "    print(\"[SKIP] Baseline visualization skipped — no Redis dump available\")\n",
    "else:\n",
    "    samples_path = Path(f'baseline/resources/datasets/{dataset_name}/{dataset_name}.samples')\n",
    "    if not samples_path.exists():\n",
    "        samples_path = Path(f'baseline/resources/datasets/{dataset_name}/{dataset_name}.sample')\n",
    "    if not samples_path.exists():\n",
    "        raise FileNotFoundError(f'Could not find samples file for {dataset_name}')\n",
    "\n",
    "    csv_path = Path(f'baseline/resources/datasets/{dataset_name}/{dataset_name}.csv')\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f'Could not find CSV file for {dataset_name}')\n",
    "\n",
    "    with csv_path.open('r') as f:\n",
    "        header_line = f.readline().strip()\n",
    "        feature_names_in_file = [name.strip() for name in header_line.split(',') if name.strip()]\n",
    "        if len(feature_names_in_file) > len(reason_df):\n",
    "            feature_names_in_file = feature_names_in_file[:-1]\n",
    "\n",
    "    samples = np.atleast_2d(np.loadtxt(samples_path, delimiter=','))\n",
    "    print(f\"[INFO] Loaded {samples.shape[0]} samples\")\n",
    "\n",
    "    feature_to_value = dict(zip(reason_df['feature'], reason_df['sample_value']))\n",
    "    feature_to_display = dict(zip(reason_df['feature'], reason_df['display_feature']))\n",
    "\n",
    "    def canonical_feature(name):\n",
    "        match = re.match(r'^[ft](\\d+)$', name)\n",
    "        return match.group(1) if match else name\n",
    "\n",
    "    feature_lookup = {}\n",
    "    for feat in feature_to_value:\n",
    "        feature_lookup[feat] = feat\n",
    "        canon = canonical_feature(feat)\n",
    "        if canon not in feature_lookup:\n",
    "            feature_lookup[canon] = feat\n",
    "\n",
    "    current_values, feature_keys_in_order, missing = [], [], []\n",
    "    for file_feat in feature_names_in_file:\n",
    "        feat_key = feature_lookup.get(file_feat) or feature_lookup.get(canonical_feature(file_feat))\n",
    "        if feat_key is None: missing.append(file_feat); continue\n",
    "        feature_keys_in_order.append(feat_key)\n",
    "        current_values.append(feature_to_value[feat_key])\n",
    "    if missing:\n",
    "        raise ValueError(f'Could not map features: {missing}')\n",
    "    current_values = np.array(current_values, dtype=float)\n",
    "\n",
    "    sample_index = next((i for i, v in enumerate(samples) if np.allclose(v, current_values, atol=1e-6)), None)\n",
    "    if sample_index is None:\n",
    "        raise ValueError('Could not find matching sample')\n",
    "    print(f\"[INFO] Matching sample index: {sample_index}\")\n",
    "\n",
    "    baseline_expl = baseline_df[baseline_df['dataset'] == dataset_name]['full_explanations'].iloc[0][sample_index]\n",
    "    baseline_expl_set = set(baseline_expl)\n",
    "\n",
    "    baseline_rows = []\n",
    "    for feat_idx, feat_key in enumerate(feature_keys_in_order):\n",
    "        display_feature = feature_to_display.get(feat_key, feat_key)\n",
    "        baseline_rows.append({'feature': feat_key, 'display_feature': display_feature,\n",
    "                               'feature_idx': feat_idx, 'in_reason': feat_idx in baseline_expl_set,\n",
    "                               'sample_value': current_values[feat_idx]})\n",
    "    baseline_df_viz = pd.DataFrame(baseline_rows)\n",
    "\n",
    "    norm_baseline_rows = []\n",
    "    for _, row in baseline_df_viz.iterrows():\n",
    "        fmin, fmax = feature_min_max(row['feature'])\n",
    "        norm_baseline_rows.append({'feature': row['feature'], 'display_feature': row['display_feature'],\n",
    "                                    'in_reason': row['in_reason'], 'sample_value': row['sample_value'],\n",
    "                                    'norm_sample': normalize_value(row['sample_value'], fmin, fmax)})\n",
    "    norm_baseline_df = pd.DataFrame(norm_baseline_rows)\n",
    "    coverage_baseline = len([r for r in norm_baseline_rows if not r['in_reason']]) / len(norm_baseline_rows) * 100\n",
    "\n",
    "    feature_order_map = {row['feature']: idx for idx, row in norm_df.iterrows()}\n",
    "    norm_baseline_df['sort_order'] = norm_baseline_df['feature'].map(feature_order_map)\n",
    "    norm_baseline_df = norm_baseline_df.sort_values('sort_order').drop(columns=['sort_order']).reset_index(drop=True)\n",
    "\n",
    "    fig_baseline = go.Figure()\n",
    "    x_pos = list(range(len(norm_baseline_df)))\n",
    "\n",
    "    in_x, in_y, ni_x, ni_y = [], [], [], []\n",
    "    for idx, row in norm_baseline_df.iterrows():\n",
    "        (in_x if row['in_reason'] else ni_x).append(idx)\n",
    "        (in_y if row['in_reason'] else ni_y).append(row['norm_sample'])\n",
    "\n",
    "    if ni_x: fig_baseline.add_trace(go.Scatter(x=ni_x, y=ni_y, mode='markers', marker=dict(color='lightgray', size=12, line=dict(width=2, color='gray')), name='Not in explanation'))\n",
    "    if in_x: fig_baseline.add_trace(go.Scatter(x=in_x, y=in_y, mode='markers', marker=dict(color='crimson', size=12, line=dict(width=2, color='darkred')), name='In explanation'))\n",
    "\n",
    "    fig_baseline.update_layout(\n",
    "        title=f'AXP Explanation<br><sub>Coverage: {coverage_baseline:.1f}%</sub>',\n",
    "        xaxis=dict(tickmode='array', tickvals=x_pos, ticktext=norm_baseline_df['display_feature'], tickangle=45),\n",
    "        yaxis=dict(range=[-0.05, 1.15]),\n",
    "        height=500, width=1200,\n",
    "    )\n",
    "    try:\n",
    "        fig_baseline.write_image('axp_explanation_ecoli_class_0_normalized.pdf')\n",
    "    except Exception as e:\n",
    "        print(f'Save failed: {e}')\n",
    "    fig_baseline.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
