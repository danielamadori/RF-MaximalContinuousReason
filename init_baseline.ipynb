{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Initialize Redis with Baseline Pre-trained Classifiers\n",
    "\n",
    "This script:\n",
    "1. Connects to Redis (clearing DBs by default)\n",
    "2. Loads a pre-trained Random Forest from baseline/Classifiers-100-converted\n",
    "3. Loads dataset from baseline/resources/datasets\n",
    "4. Loads test samples from baseline/resources/datasets\n",
    "5. Stores everything in Redis (Dataset, Forest, Endpoints, Initial Candidate)\n",
    "\n",
    "Directory Structure:\n",
    "- Classifiers: baseline/Classifiers-100-converted/<dataset_name>/*.json\n",
    "- Datasets: baseline/resources/datasets/<dataset_name>/<dataset_name>.csv\n",
    "- Samples: baseline/resources/datasets/<dataset_name>/<dataset_name>.samples\n",
    "\n",
    "Usage:\n",
    "    python init_baseline.py --list-datasets\n",
    "    python init_baseline.py iris --class-label \"0\"\n",
    "    python init_baseline.py sonar --class-label \"1\" --test-sample-index \"0,5-8,20\"\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import redis\n",
    "import json\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Shared modules\n",
    "from redis_helpers.connection import connect_redis\n",
    "from redis_helpers.utils import clean_all_databases\n",
    "from init_utils import (\n",
    "    store_forest_and_endpoints,\n",
    "    initialize_seed_candidate,\n",
    "    store_dataset_total_samples\n",
    ")\n",
    "from helpers import convert_numpy_types, parse_sample_indices\n",
    "from load_rf_from_json import load_rf_from_json\n",
    "from baseline.xrf import Dataset\n",
    "from rf_utils import sklearn_forest_to_forest\n",
    "\n",
    "# Constants\n",
    "CLASSIFIERS_ROOT = os.path.join('baseline', 'Classifiers-100-converted')\n",
    "DATASETS_ROOT = os.path.join('baseline', 'resources', 'datasets')\n",
    "\n",
    "\n",
    "def list_available_datasets():\n",
    "    \"\"\"List all datasets with pre-trained classifiers in baseline directory.\"\"\"\n",
    "    print(\"\\nAvailable Baseline Datasets:\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    if not os.path.exists(CLASSIFIERS_ROOT):\n",
    "        print(f\"[ERROR] Classifiers directory not found: {CLASSIFIERS_ROOT}\")\n",
    "        return\n",
    "\n",
    "    datasets = []\n",
    "    for name in sorted(os.listdir(CLASSIFIERS_ROOT)):\n",
    "        dataset_dir = os.path.join(CLASSIFIERS_ROOT, name)\n",
    "        if not os.path.isdir(dataset_dir):\n",
    "            continue\n",
    "\n",
    "        # Check if dataset files exist\n",
    "        csv_path = os.path.join(DATASETS_ROOT, name, f\"{name}.csv\")\n",
    "        samples_path = os.path.join(DATASETS_ROOT, name, f\"{name}.samples\")\n",
    "\n",
    "        # Count JSON classifiers\n",
    "        json_files = [f for f in os.listdir(dataset_dir) if f.endswith('.json')]\n",
    "\n",
    "        status = \"✓\"\n",
    "        notes = []\n",
    "        if not os.path.exists(csv_path):\n",
    "            status = \"✗\"\n",
    "            notes.append(\"CSV missing\")\n",
    "        if not os.path.exists(samples_path):\n",
    "            status = \"✗\"\n",
    "            notes.append(\"samples missing\")\n",
    "        if len(json_files) == 0:\n",
    "            status = \"✗\"\n",
    "            notes.append(\"no classifiers\")\n",
    "\n",
    "        note_str = f\" ({', '.join(notes)})\" if notes else \"\"\n",
    "        print(f\"  {status} {name:<30} {len(json_files)} classifier(s){note_str}\")\n",
    "\n",
    "        if status == \"✓\":\n",
    "            datasets.append(name)\n",
    "\n",
    "    print(f\"\\nTotal: {len(datasets)} datasets ready to use\")\n",
    "    print(\"\\nUsage: python init_baseline.py <dataset_name> --class-label <label>\")\n",
    "\n",
    "\n",
    "def find_classifier_json(dataset_name):\n",
    "    \"\"\"\n",
    "    Find all classifier JSON files for a dataset.\n",
    "\n",
    "    Returns:\n",
    "        List of paths to JSON classifier files\n",
    "    \"\"\"\n",
    "    classifier_dir = os.path.join(CLASSIFIERS_ROOT, dataset_name)\n",
    "\n",
    "    # Try finding directory with hyphens if original not found\n",
    "    if not os.path.exists(classifier_dir) and '_' in dataset_name:\n",
    "        alt_name = dataset_name.replace('_', '-')\n",
    "        alt_dir = os.path.join(CLASSIFIERS_ROOT, alt_name)\n",
    "        if os.path.exists(alt_dir):\n",
    "            classifier_dir = alt_dir\n",
    "            # We don't change dataset_name here as it might be used for filtering inside\n",
    "\n",
    "\n",
    "    if not os.path.exists(classifier_dir):\n",
    "        return []\n",
    "\n",
    "    json_files = [\n",
    "        os.path.join(classifier_dir, fname)\n",
    "        for fname in os.listdir(classifier_dir)\n",
    "        if fname.endswith('.json')\n",
    "    ]\n",
    "\n",
    "    return sorted(json_files)\n",
    "\n",
    "\n",
    "def load_dataset_from_baseline(dataset_name, separator=','):\n",
    "    \"\"\"\n",
    "    Load dataset CSV and samples from baseline directory structure.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: Name of the dataset\n",
    "        separator: CSV separator (default: ',')\n",
    "\n",
    "    Returns:\n",
    "        (X_train, X_test, y_train, y_test, feature_names, all_classes)\n",
    "    \"\"\"\n",
    "    # Handle potential name mismatch (underscore vs hyphen)\n",
    "    actual_name = dataset_name\n",
    "    dataset_dir = os.path.join(DATASETS_ROOT, dataset_name)\n",
    "\n",
    "    if not os.path.exists(dataset_dir) and '_' in dataset_name:\n",
    "        alt_name = dataset_name.replace('_', '-')\n",
    "        if os.path.exists(os.path.join(DATASETS_ROOT, alt_name)):\n",
    "            actual_name = alt_name\n",
    "\n",
    "    dataset_path = os.path.join(DATASETS_ROOT, actual_name, f\"{actual_name}.csv\")\n",
    "    samples_path = os.path.join(DATASETS_ROOT, actual_name, f\"{actual_name}.samples\")\n",
    "\n",
    "    if not os.path.exists(dataset_path):\n",
    "        raise FileNotFoundError(f\"Dataset CSV not found: {dataset_path}\")\n",
    "\n",
    "    if not os.path.exists(samples_path):\n",
    "        raise FileNotFoundError(f\"Samples file not found: {samples_path}\")\n",
    "\n",
    "    if os.path.getsize(samples_path) == 0:\n",
    "        raise ValueError(f\"Samples file is empty: {samples_path}\")\n",
    "\n",
    "    # Load dataset using baseline Dataset class\n",
    "    print(f\"[INFO] Loading dataset from: {dataset_path}\")\n",
    "    data = Dataset(filename=dataset_path, separator=separator, use_categorical=False)\n",
    "\n",
    "    # Get train/test split from the Dataset\n",
    "    X_train_raw, X_test_raw, y_train, y_test = data.train_test_split()\n",
    "\n",
    "    # Transform data (apply any transformations the Dataset class has)\n",
    "    X_train = data.transform(X_train_raw)\n",
    "    X_test = data.transform(X_test_raw)\n",
    "\n",
    "    # Load samples - these will be our actual test samples\n",
    "    print(f\"[INFO] Loading samples from: {samples_path}\")\n",
    "    samples = np.loadtxt(samples_path, delimiter=separator)\n",
    "    samples = np.atleast_2d(samples)\n",
    "\n",
    "    # Validate sample dimensions\n",
    "    expected_features = len(data.features)\n",
    "    if samples.shape[1] == expected_features + 1:\n",
    "        print(\"[INFO] Sample file includes labels; dropping last column.\")\n",
    "        samples = samples[:, :-1]\n",
    "    elif samples.shape[1] != expected_features:\n",
    "        raise ValueError(\n",
    "            f\"Sample file feature count mismatch: expected {expected_features}, \"\n",
    "            f\"found {samples.shape[1]} in {samples_path}\"\n",
    "        )\n",
    "\n",
    "    print(f\"[INFO] Loaded {len(samples)} test samples\")\n",
    "    print(f\"[INFO] Training set: {len(X_train)} samples\")\n",
    "    print(f\"[INFO] Features: {data.features}\")\n",
    "    print(f\"[INFO] Classes: {np.unique(y_train)}\")\n",
    "\n",
    "    # Convert class labels to strings for consistency (ensure int format '0' not '0.0')\n",
    "    y_train = y_train.astype(int).astype(str)\n",
    "    y_test = y_test.astype(int).astype(str)\n",
    "\n",
    "    return X_train, samples, y_train, None, data.features, np.unique(y_train), data\n",
    "\n",
    "\n",
    "def load_classifier_from_json(dataset_name, classifier_index=0):\n",
    "    \"\"\"\n",
    "    Load a pre-trained classifier from JSON.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: Name of the dataset\n",
    "        classifier_index: Index of classifier to use if multiple exist (default: 0)\n",
    "\n",
    "    Returns:\n",
    "        (sklearn_rf, classifier_path) tuple\n",
    "    \"\"\"\n",
    "    json_files = find_classifier_json(dataset_name)\n",
    "\n",
    "    if not json_files:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No classifier JSON files found for dataset '{dataset_name}' \"\n",
    "            f\"in {os.path.join(CLASSIFIERS_ROOT, dataset_name)}\"\n",
    "        )\n",
    "\n",
    "    if classifier_index >= len(json_files):\n",
    "        raise ValueError(\n",
    "            f\"Classifier index {classifier_index} out of range. \"\n",
    "            f\"Found {len(json_files)} classifier(s).\"\n",
    "        )\n",
    "\n",
    "    classifier_path = json_files[classifier_index]\n",
    "\n",
    "    print(f\"[INFO] Loading pre-trained classifier: {os.path.basename(classifier_path)}\")\n",
    "\n",
    "    # Parse parameters from filename\n",
    "    filename = os.path.basename(classifier_path)\n",
    "    try:\n",
    "        parts = filename.split('_nbestim_')[1]\n",
    "        n_estimators = int(parts.split('_maxdepth_')[0])\n",
    "        max_depth = int(parts.split('_maxdepth_')[1].split('.')[0])\n",
    "        print(f\"[INFO] Classifier: {n_estimators} trees, max_depth={max_depth}\")\n",
    "    except (IndexError, ValueError):\n",
    "        print(f\"[WARNING] Could not parse classifier parameters from filename\")\n",
    "\n",
    "    # Load the classifier using load_rf_from_json\n",
    "    sklearn_rf = load_rf_from_json(classifier_path)\n",
    "\n",
    "    print(f\"[INFO] Successfully loaded classifier with {sklearn_rf.n_estimators} trees\")\n",
    "\n",
    "    return sklearn_rf, classifier_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b044204bd8b37d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Redis DB 0 (DATA) on port 6379\n",
      "Connected to Redis DB 1 (CAN) on port 6379\n",
      "Connected to Redis DB 2 (R) on port 6379\n",
      "Connected to Redis DB 3 (NR) on port 6379\n",
      "Connected to Redis DB 4 (CAR) on port 6379\n",
      "Connected to Redis DB 5 (AR) on port 6379\n",
      "Connected to Redis DB 6 (GP) on port 6379\n",
      "Connected to Redis DB 7 (BP) on port 6379\n",
      "Connected to Redis DB 8 (PR) on port 6379\n",
      "Connected to Redis DB 9 (AP) on port 6379\n",
      "Connected to Redis DB 10 (LOGS) on port 6379\n",
      "Established 11 Redis connections\n"
     ]
    }
   ],
   "source": [
    "\n",
    "connections, db_mapping = connect_redis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8cd122928476e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n"
     ]
    }
   ],
   "source": [
    "clean_all_databases(connections, db_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7423e18b1c215cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\ann-thyroid\\ann-thyroid.csv\n",
      "c nof features: 21\n",
      "c nof classes: 3\n",
      "c nof samples: 7129\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\ann-thyroid\\ann-thyroid.samples\n",
      "[INFO] Loaded 720 test samples\n",
      "[INFO] Training set: 5703 samples\n",
      "[INFO] Features: ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A20', 'A21']\n",
      "[INFO] Classes: [1. 2. 3.]\n",
      "[INFO] Loading pre-trained classifier: ann-thyroid_nbestim_100_maxdepth_4.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=4\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 20 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 20 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "ann-thyroid 1 over 720 samples not validated\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\appendicitis\\appendicitis.csv\n",
      "c nof features: 7\n",
      "c nof classes: 2\n",
      "c nof samples: 106\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\appendicitis\\appendicitis.samples\n",
      "[INFO] Loaded 43 test samples\n",
      "[INFO] Training set: 84 samples\n",
      "[INFO] Features: ['At1', 'At2', 'At3', 'At4', 'At5', 'At6', 'At7']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: appendicitis_nbestim_100_maxdepth_6.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=6\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 7 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 7 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "appendicitis FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\banknote\\banknote.csv\n",
      "c nof features: 4\n",
      "c nof classes: 2\n",
      "c nof samples: 1348\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\banknote\\banknote.samples\n",
      "[INFO] Loaded 138 test samples\n",
      "[INFO] Training set: 1078 samples\n",
      "[INFO] Features: ['feat1', 'feat2', 'feat3', 'feat4']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: banknote_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 4 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 4 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "banknote FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\biodegradation\\biodegradation.csv\n",
      "c nof features: 41\n",
      "c nof classes: 2\n",
      "c nof samples: 1052\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\biodegradation\\biodegradation.samples\n",
      "[INFO] Loaded 106 test samples\n",
      "[INFO] Training set: 841 samples\n",
      "[INFO] Features: ['Feat0', 'Feat1', 'Feat2', 'Feat3', 'Feat4', 'Feat5', 'Feat6', 'Feat7', 'Feat8', 'Feat9', 'Feat10', 'Feat11', 'Feat12', 'Feat13', 'Feat14', 'Feat15', 'Feat16', 'Feat17', 'Feat18', 'Feat19', 'Feat20', 'Feat21', 'Feat22', 'Feat23', 'Feat24', 'Feat25', 'Feat26', 'Feat27', 'Feat28', 'Feat29', 'Feat30', 'Feat31', 'Feat32', 'Feat33', 'Feat34', 'Feat35', 'Feat36', 'Feat37', 'Feat38', 'Feat39', 'Feat40']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: biodegradation_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 39 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 39 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "biodegradation FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\ecoli\\ecoli.csv\n",
      "c nof features: 7\n",
      "c nof classes: 5\n",
      "c nof samples: 327\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\ecoli\\ecoli.samples\n",
      "[INFO] Loaded 66 test samples\n",
      "[INFO] Training set: 261 samples\n",
      "[INFO] Features: ['mcg', 'gvh', 'lip', 'chg', 'aac', 'alm1', 'alm2']\n",
      "[INFO] Classes: [0. 1. 4. 5. 7.]\n",
      "[INFO] Loading pre-trained classifier: ecoli_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 6 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 6 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "ecoli FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\glass2\\glass2.csv\n",
      "c nof features: 9\n",
      "c nof classes: 2\n",
      "c nof samples: 162\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\glass2\\glass2.samples\n",
      "[INFO] Loaded 66 test samples\n",
      "[INFO] Training set: 129 samples\n",
      "[INFO] Features: ['Refractive Index', 'Sodium', 'Magnesium', 'Aluminum', 'Silicon', 'Potassium', 'Calcium', 'Barium', 'Iron']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: glass2_nbestim_100_maxdepth_6.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=6\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 9 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 9 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "glass2 FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\heart-c\\heart-c.csv\n",
      "c nof features: 13\n",
      "c nof classes: 2\n",
      "c nof samples: 302\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\heart-c\\heart-c.samples\n",
      "[INFO] Loaded 61 test samples\n",
      "[INFO] Training set: 241 samples\n",
      "[INFO] Features: ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: heart-c_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 13 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 13 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "heart-c FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\ionosphere\\ionosphere.csv\n",
      "c nof features: 34\n",
      "c nof classes: 2\n",
      "c nof samples: 350\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\ionosphere\\ionosphere.samples\n",
      "[INFO] Loaded 71 test samples\n",
      "[INFO] Training set: 280 samples\n",
      "[INFO] Features: ['Feat1', 'Feat2', 'Feat3', 'Feat4', 'Feat5', 'Feat6', 'Feat7', 'Feat8', 'Feat9', 'Feat10', 'Feat11', 'Feat12', 'Feat13', 'Feat14', 'Feat15', 'Feat16', 'Feat17', 'Feat18', 'Feat19', 'Feat20', 'Feat21', 'Feat22', 'Feat23', 'Feat24', 'Feat25', 'Feat26', 'Feat27', 'Feat28', 'Feat29', 'Feat30', 'Feat31', 'Feat32', 'Feat33', 'Feat34']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: ionosphere_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 33 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 33 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "ionosphere FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\iris\\iris.csv\n",
      "c nof features: 4\n",
      "c nof classes: 3\n",
      "c nof samples: 149\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\iris\\iris.samples\n",
      "[INFO] Loaded 60 test samples\n",
      "[INFO] Training set: 119 samples\n",
      "[INFO] Features: ['\"sepal.length\"', '\"sepal.width\"', '\"petal.length\"', '\"petal.width\"']\n",
      "[INFO] Classes: [0. 1. 2.]\n",
      "[INFO] Loading pre-trained classifier: iris_nbestim_100_maxdepth_6.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=6\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 4 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 4 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "iris FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\karhunen\\karhunen.csv\n",
      "c nof features: 64\n",
      "c nof classes: 10\n",
      "c nof samples: 1994\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\karhunen\\karhunen.samples\n",
      "[INFO] Loaded 200 test samples\n",
      "[INFO] Training set: 1595 samples\n",
      "[INFO] Features: ['att1', 'att2', 'att3', 'att4', 'att5', 'att6', 'att7', 'att8', 'att9', 'att10', 'att11', 'att12', 'att13', 'att14', 'att15', 'att16', 'att17', 'att18', 'att19', 'att20', 'att21', 'att22', 'att23', 'att24', 'att25', 'att26', 'att27', 'att28', 'att29', 'att30', 'att31', 'att32', 'att33', 'att34', 'att35', 'att36', 'att37', 'att38', 'att39', 'att40', 'att41', 'att42', 'att43', 'att44', 'att45', 'att46', 'att47', 'att48', 'att49', 'att50', 'att51', 'att52', 'att53', 'att54', 'att55', 'att56', 'att57', 'att58', 'att59', 'att60', 'att61', 'att62', 'att63', 'att64']\n",
      "[INFO] Classes: [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "[INFO] Loading pre-trained classifier: karhunen_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 64 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 64 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "karhunen 1 over 200 samples not validated\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\letter\\letter.csv\n",
      "c nof features: 16\n",
      "c nof classes: 26\n",
      "c nof samples: 18668\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\letter\\letter.samples\n",
      "[INFO] Loaded 400 test samples\n",
      "[INFO] Training set: 14934 samples\n",
      "[INFO] Features: ['Feat1', 'Feat2', 'Feat3', 'Feat4', 'Feat5', 'Feat6', 'Feat7', 'Feat8', 'Feat9', 'Feat10', 'Feat11', 'Feat12', 'Feat13', 'Feat14', 'Feat15', 'Feat16']\n",
      "[INFO] Classes: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25.]\n",
      "[INFO] Loading pre-trained classifier: letter_nbestim_100_maxdepth_8.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=8\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 16 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 16 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "letter 5 over 400 samples not validated\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\magic\\magic.csv\n",
      "c nof features: 10\n",
      "c nof classes: 2\n",
      "c nof samples: 18905\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\magic\\magic.samples\n",
      "[INFO] Loaded 381 test samples\n",
      "[INFO] Training set: 15124 samples\n",
      "[INFO] Features: ['FLength', 'FWidth', 'FSize', 'FConc', 'FConc1', 'FAsym', 'FM3Long', 'FM3Trans', 'FAlpha', 'FDist']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: magic_nbestim_100_maxdepth_6.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=6\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 10 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 10 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "magic FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\mofn-3-7-10\\mofn-3-7-10.csv\n",
      "c nof features: 10\n",
      "c nof classes: 2\n",
      "c nof samples: 1024\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\mofn-3-7-10\\mofn-3-7-10.samples\n",
      "[INFO] Loaded 133 test samples\n",
      "[INFO] Training set: 819 samples\n",
      "[INFO] Features: ['Bit-0', 'Bit-1', 'Bit-2', 'Bit-3', 'Bit-4', 'Bit-5', 'Bit-6', 'Bit-7', 'Bit-8', 'Bit-9']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: mofn-3-7-10_nbestim_100_maxdepth_6.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=6\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 10 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 10 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "mofn-3-7-10 FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\new-thyroid\\new-thyroid.csv\n",
      "c nof features: 5\n",
      "c nof classes: 3\n",
      "c nof samples: 215\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\new-thyroid\\new-thyroid.samples\n",
      "[INFO] Loaded 43 test samples\n",
      "[INFO] Training set: 172 samples\n",
      "[INFO] Features: ['2', '3', '4', '5', '6']\n",
      "[INFO] Classes: [1. 2. 3.]\n",
      "[INFO] Loading pre-trained classifier: new-thyroid_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 5 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 5 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "new-thyroid FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\pendigits\\pendigits.csv\n",
      "c nof features: 16\n",
      "c nof classes: 10\n",
      "c nof samples: 10992\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\pendigits\\pendigits.samples\n",
      "[INFO] Loaded 220 test samples\n",
      "[INFO] Training set: 8793 samples\n",
      "[INFO] Features: ['Feat1', 'Feat2', 'Feat3', 'Feat4', 'Feat5', 'Feat6', 'Feat7', 'Feat8', 'Feat9', 'Feat10', 'Feat11', 'Feat12', 'Feat13', 'Feat14', 'Feat15', 'Feat16']\n",
      "[INFO] Classes: [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "[INFO] Loading pre-trained classifier: pendigits_nbestim_100_maxdepth_6.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=6\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 16 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 16 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "pendigits FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\phoneme\\phoneme.csv\n",
      "c nof features: 5\n",
      "c nof classes: 2\n",
      "c nof samples: 5349\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\phoneme\\phoneme.samples\n",
      "[INFO] Loaded 541 test samples\n",
      "[INFO] Training set: 4279 samples\n",
      "[INFO] Features: ['Aa', 'Ao', 'Dcl', 'Iy', 'Sh']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: phoneme_nbestim_100_maxdepth_6.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=6\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 5 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 5 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "phoneme 4 over 541 samples not validated\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\ring\\ring.csv\n",
      "c nof features: 20\n",
      "c nof classes: 2\n",
      "c nof samples: 7400\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\ring\\ring.samples\n",
      "[INFO] Loaded 740 test samples\n",
      "[INFO] Training set: 5920 samples\n",
      "[INFO] Features: ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A20']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: ring_nbestim_100_maxdepth_6.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=6\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 20 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 20 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "ring 2 over 740 samples not validated\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\segmentation\\segmentation.csv\n",
      "c nof features: 19\n",
      "c nof classes: 7\n",
      "c nof samples: 210\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\segmentation\\segmentation.samples\n",
      "[INFO] Loaded 42 test samples\n",
      "[INFO] Training set: 168 samples\n",
      "[INFO] Features: ['Feat1', 'Feat2', 'Feat3', 'Feat4', 'Feat5', 'Feat6', 'Feat7', 'Feat8', 'Feat9', 'Feat10', 'Feat11', 'Feat12', 'Feat13', 'Feat14', 'Feat15', 'Feat16', 'Feat17', 'Feat18', 'Feat19']\n",
      "[INFO] Classes: [0. 1. 2. 3. 4. 5. 6.]\n",
      "[INFO] Loading pre-trained classifier: segmentation_nbestim_100_maxdepth_4.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=4\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 18 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 18 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "segmentation FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\shuttle\\shuttle.csv\n",
      "c nof features: 9\n",
      "c nof classes: 7\n",
      "c nof samples: 58000\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\shuttle\\shuttle.samples\n",
      "[INFO] Loaded 1160 test samples\n",
      "[INFO] Training set: 46400 samples\n",
      "[INFO] Features: ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9']\n",
      "[INFO] Classes: [1. 2. 3. 4. 5. 6. 7.]\n",
      "[INFO] Loading pre-trained classifier: shuttle_nbestim_100_maxdepth_3.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=3\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 9 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 9 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "shuttle FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\sonar\\sonar.csv\n",
      "c nof features: 60\n",
      "c nof classes: 2\n",
      "c nof samples: 208\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\sonar\\sonar.samples\n",
      "[INFO] Loaded 42 test samples\n",
      "[INFO] Training set: 166 samples\n",
      "[INFO] Features: ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A23', 'A24', 'A25', 'A26', 'A27', 'A28', 'A29', 'A30', 'A31', 'A32', 'A33', 'A34', 'A35', 'A36', 'A37', 'A38', 'A39', 'A40', 'A41', 'A42', 'A43', 'A44', 'A45', 'A46', 'A47', 'A48', 'A49', 'A50', 'A51', 'A52', 'A53', 'A54', 'A55', 'A56', 'A57', 'A58', 'A59', 'A60']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: sonar_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 60 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 60 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "sonar FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\spambase\\spambase.csv\n",
      "c nof features: 57\n",
      "c nof classes: 2\n",
      "c nof samples: 4210\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\spambase\\spambase.samples\n",
      "[INFO] Loaded 461 test samples\n",
      "[INFO] Training set: 3368 samples\n",
      "[INFO] Features: ['Feat0', 'Feat1', 'Feat2', 'Feat3', 'Feat4', 'Feat5', 'Feat6', 'Feat7', 'Feat8', 'Feat9', 'Feat10', 'Feat11', 'Feat12', 'Feat13', 'Feat14', 'Feat15', 'Feat16', 'Feat17', 'Feat18', 'Feat19', 'Feat20', 'Feat21', 'Feat22', 'Feat23', 'Feat24', 'Feat25', 'Feat26', 'Feat27', 'Feat28', 'Feat29', 'Feat30', 'Feat31', 'Feat32', 'Feat33', 'Feat34', 'Feat35', 'Feat36', 'Feat37', 'Feat38', 'Feat39', 'Feat40', 'Feat41', 'Feat42', 'Feat43', 'Feat44', 'Feat45', 'Feat46', 'Feat47', 'Feat48', 'Feat49', 'Feat50', 'Feat51', 'Feat52', 'Feat53', 'Feat54', 'Feat55', 'Feat56']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: spambase_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 55 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 55 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "spambase 4 over 461 samples not validated\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\spectf\\spectf.csv\n",
      "c nof features: 44\n",
      "c nof classes: 2\n",
      "c nof samples: 267\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\spectf\\spectf.samples\n",
      "[INFO] Loaded 54 test samples\n",
      "[INFO] Training set: 213 samples\n",
      "[INFO] Features: ['\"F1R\"', '\"F1S\"', '\"F2R\"', '\"F2S\"', '\"F3R\"', '\"F3S\"', '\"F4R\"', '\"F4S\"', '\"F5R\"', '\"F5S\"', '\"F6R\"', '\"F6S\"', '\"F7R\"', '\"F7S\"', '\"F8R\"', '\"F8S\"', '\"F9R\"', '\"F9S\"', '\"F10R\"', '\"F10S\"', '\"F11R\"', '\"F11S\"', '\"F12R\"', '\"F12S\"', '\"F13R\"', '\"F13S\"', '\"F14R\"', '\"F14S\"', '\"F15R\"', '\"F15S\"', '\"F16R\"', '\"F16S\"', '\"F17R\"', '\"F17S\"', '\"F18R\"', '\"F18S\"', '\"F19R\"', '\"F19S\"', '\"F20R\"', '\"F20S\"', '\"F21R\"', '\"F21S\"', '\"F22R\"', '\"F22S\"']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: spectf_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 44 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 44 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "spectf FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\texture\\texture.csv\n",
      "c nof features: 40\n",
      "c nof classes: 11\n",
      "c nof samples: 5473\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\texture\\texture.samples\n",
      "[INFO] Loaded 550 test samples\n",
      "[INFO] Training set: 4378 samples\n",
      "[INFO] Features: ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A20', 'A21', 'A22', 'A23', 'A24', 'A25', 'A26', 'A27', 'A28', 'A29', 'A30', 'A31', 'A32', 'A33', 'A34', 'A35', 'A36', 'A37', 'A38', 'A39', 'A40']\n",
      "[INFO] Classes: [ 2.  3.  4.  6.  7.  8.  9. 10. 12. 13. 14.]\n",
      "[INFO] Loading pre-trained classifier: texture_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 40 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 40 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "texture 1 over 550 samples not validated\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\threeOf9\\threeOf9.csv\n",
      "c nof features: 9\n",
      "c nof classes: 2\n",
      "c nof samples: 512\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\threeOf9\\threeOf9.samples\n",
      "[INFO] Loaded 103 test samples\n",
      "[INFO] Training set: 409 samples\n",
      "[INFO] Features: ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: threeOf9_nbestim_100_maxdepth_3.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=3\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 9 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 9 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "threeOf9 FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\twonorm\\twonorm.csv\n",
      "c nof features: 20\n",
      "c nof classes: 2\n",
      "c nof samples: 7400\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\twonorm\\twonorm.samples\n",
      "[INFO] Loaded 740 test samples\n",
      "[INFO] Training set: 5920 samples\n",
      "[INFO] Features: ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A20']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: twonorm_nbestim_100_maxdepth_3.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=3\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 20 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 20 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "twonorm 5 over 740 samples not validated\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\vowel\\vowel.csv\n",
      "c nof features: 13\n",
      "c nof classes: 11\n",
      "c nof samples: 990\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\vowel\\vowel.samples\n",
      "[INFO] Loaded 198 test samples\n",
      "[INFO] Training set: 792 samples\n",
      "[INFO] Features: ['Train or Test', 'Speaker Number', 'Sex', 'Feature 0', 'Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5', 'Feature 6', 'Feature 7', 'Feature 8', 'Feature 9']\n",
      "[INFO] Classes: [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      "[INFO] Loading pre-trained classifier: vowel_nbestim_100_maxdepth_6.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=6\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 13 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 13 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "vowel FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\waveform-21\\waveform-21.csv\n",
      "c nof features: 21\n",
      "c nof classes: 3\n",
      "c nof samples: 5000\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\waveform-21\\waveform-21.samples\n",
      "[INFO] Loaded 500 test samples\n",
      "[INFO] Training set: 4000 samples\n",
      "[INFO] Features: ['X00', 'X01', 'X02', 'X03', 'X04', 'X05', 'X06', 'X07', 'X08', 'X09', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20']\n",
      "[INFO] Classes: [0. 1. 2.]\n",
      "[INFO] Loading pre-trained classifier: waveform-21_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 21 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 21 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "waveform-21 1 over 500 samples not validated\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\waveform-40\\waveform-40.csv\n",
      "c nof features: 40\n",
      "c nof classes: 3\n",
      "c nof samples: 5000\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\waveform-40\\waveform-40.samples\n",
      "[INFO] Loaded 500 test samples\n",
      "[INFO] Training set: 4000 samples\n",
      "[INFO] Features: ['X00', 'X01', 'X02', 'X03', 'X04', 'X05', 'X06', 'X07', 'X08', 'X09', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39']\n",
      "[INFO] Classes: [0. 1. 2.]\n",
      "[INFO] Loading pre-trained classifier: waveform-40_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 40 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 40 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "waveform-40 2 over 500 samples not validated\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\wdbc\\wdbc.csv\n",
      "c nof features: 30\n",
      "c nof classes: 2\n",
      "c nof samples: 569\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\wdbc\\wdbc.samples\n",
      "[INFO] Loaded 114 test samples\n",
      "[INFO] Training set: 455 samples\n",
      "[INFO] Features: ['Feat1', 'Feat2', 'Feat3', 'Feat4', 'Feat5', 'Feat6', 'Feat7', 'Feat8', 'Feat9', 'Feat10', 'Feat11', 'Feat12', 'Feat13', 'Feat14', 'Feat15', 'Feat16', 'Feat17', 'Feat18', 'Feat19', 'Feat20', 'Feat21', 'Feat22', 'Feat23', 'Feat24', 'Feat25', 'Feat26', 'Feat27', 'Feat28', 'Feat29', 'Feat30']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: wdbc_nbestim_100_maxdepth_4.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=4\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 30 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 30 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "wdbc FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\wine-recog\\wine-recog.csv\n",
      "c nof features: 13\n",
      "c nof classes: 3\n",
      "c nof samples: 178\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\wine-recog\\wine-recog.samples\n",
      "[INFO] Loaded 72 test samples\n",
      "[INFO] Training set: 142 samples\n",
      "[INFO] Features: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12']\n",
      "[INFO] Classes: [1. 2. 3.]\n",
      "[INFO] Loading pre-trained classifier: wine-recog_nbestim_100_maxdepth_3.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=3\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 13 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 13 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "wine-recog FULLY VALIDATED\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\wpbc\\wpbc.csv\n",
      "c nof features: 33\n",
      "c nof classes: 2\n",
      "c nof samples: 194\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\wpbc\\wpbc.samples\n",
      "[INFO] Loaded 78 test samples\n",
      "[INFO] Training set: 155 samples\n",
      "[INFO] Features: ['Feat1', 'Feat2', 'Feat3', 'Feat4', 'Feat5', 'Feat6', 'Feat7', 'Feat8', 'Feat9', 'Feat10', 'Feat11', 'Feat12', 'Feat13', 'Feat14', 'Feat15', 'Feat16', 'Feat17', 'Feat18', 'Feat19', 'Feat20', 'Feat21', 'Feat22', 'Feat23', 'Feat24', 'Feat25', 'Feat26', 'Feat27', 'Feat28', 'Feat29', 'Feat30', 'Feat31', 'Feat32', 'Feat33']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: wpbc_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 33 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 33 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "wpbc 4 over 78 samples not validated\n",
      "########################################\n",
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n",
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\xd6\\xd6.csv\n",
      "c nof features: 9\n",
      "c nof classes: 2\n",
      "c nof samples: 512\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\xd6\\xd6.samples\n",
      "[INFO] Loaded 195 test samples\n",
      "[INFO] Training set: 409 samples\n",
      "[INFO] Features: ['Attribute 1', 'Attribute 2', 'Attribute 3', 'Attribute 4', 'Attribute 5', 'Attribute 6', 'Attribute 7', 'Attribute 8', 'Attribute 9']\n",
      "[INFO] Classes: [0. 1.]\n",
      "[INFO] Loading pre-trained classifier: xd6_nbestim_100_maxdepth_6.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=6\n",
      "[INFO] Successfully loaded classifier with 100 trees\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 9 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 9 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "########################################\n",
      "xd6 FULLY VALIDATED\n",
      "########################################\n"
     ]
    }
   ],
   "source": [
    "from rcheck_cache import rcheck_cache, saturate\n",
    "\n",
    "DATASETS = ['ann-thyroid', 'appendicitis', 'banknote', 'biodegradation', 'ecoli', 'glass2', 'heart-c', 'ionosphere', 'iris', 'karhunen', 'letter', 'magic', 'mofn-3-7-10', 'new-thyroid', 'pendigits', 'phoneme', 'ring', 'segmentation', 'shuttle', 'sonar', 'spambase', 'spectf', 'texture', 'threeOf9', 'twonorm', 'vowel', 'waveform-21', 'waveform-40', 'wdbc', 'wine-recog', 'wpbc', 'xd6']\n",
    "\n",
    "for dataset_name in DATASETS:\n",
    "    clean_all_databases(connections, db_mapping)\n",
    "    X_train, X_test_samples, y_train, _, feature_names, all_classes, data = load_dataset_from_baseline(dataset_name)\n",
    "    classifier_index = 0\n",
    "    sklearn_rf, classifier_path = load_classifier_from_json(dataset_name, classifier_index)\n",
    "    our_forest = sklearn_forest_to_forest(sklearn_rf, feature_names)\n",
    "    eu_data = store_forest_and_endpoints(connections, our_forest)\n",
    "    X_test = [dict(zip(feature_names,  x_test) ) for x_test in X_test_samples]\n",
    "    predictions = [our_forest.predict(x_test) for x_test in X_test ]\n",
    "    validated= True\n",
    "    not_validated = []\n",
    "    validated_test = []\n",
    "    for i in range(len(X_test)):\n",
    "        nodes = []\n",
    "        for tree in our_forest.trees:\n",
    "            nodes.append(tree.root)\n",
    "        caches = {\n",
    "            'R': set(),\n",
    "            'NR': set(),\n",
    "            'GP': set(),\n",
    "            'BP': set(),\n",
    "            'AR': set(),\n",
    "            'AP': set()\n",
    "        }\n",
    "        icf = our_forest.extract_icf(X_test[i])\n",
    "        if not rcheck_cache(\n",
    "                    connections=connections,\n",
    "                    icf=icf,\n",
    "                    label=predictions[i],\n",
    "                    nodes=saturate(icf, nodes),\n",
    "                    eu_data=eu_data,\n",
    "                    forest=our_forest,\n",
    "                    caches=caches,\n",
    "                    info={}\n",
    "                ):\n",
    "            not_validated.append(icf)\n",
    "            validated = False\n",
    "        else:\n",
    "            validated_test.append(X_test[i])\n",
    "\n",
    "    print(40*\"#\")\n",
    "    if not validated:\n",
    "        print(f\"{dataset_name} {len(not_validated)} over {len(X_test)} samples not validated\")\n",
    "    else:\n",
    "        print(f\"{dataset_name} FULLY VALIDATED\")\n",
    "    print(40*\"#\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "353d424f3bfa5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"karhunen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "612720d6eaf35616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading dataset from: baseline\\resources\\datasets\\karhunen\\karhunen.csv\n",
      "c nof features: 64\n",
      "c nof classes: 10\n",
      "c nof samples: 1994\n",
      "[INFO] Loading samples from: baseline\\resources\\datasets\\karhunen\\karhunen.samples\n",
      "[INFO] Loaded 200 test samples\n",
      "[INFO] Training set: 1595 samples\n",
      "[INFO] Features: ['att1', 'att2', 'att3', 'att4', 'att5', 'att6', 'att7', 'att8', 'att9', 'att10', 'att11', 'att12', 'att13', 'att14', 'att15', 'att16', 'att17', 'att18', 'att19', 'att20', 'att21', 'att22', 'att23', 'att24', 'att25', 'att26', 'att27', 'att28', 'att29', 'att30', 'att31', 'att32', 'att33', 'att34', 'att35', 'att36', 'att37', 'att38', 'att39', 'att40', 'att41', 'att42', 'att43', 'att44', 'att45', 'att46', 'att47', 'att48', 'att49', 'att50', 'att51', 'att52', 'att53', 'att54', 'att55', 'att56', 'att57', 'att58', 'att59', 'att60', 'att61', 'att62', 'att63', 'att64']\n",
      "[INFO] Classes: [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test_samples, y_train, _, feature_names, all_classes, data = load_dataset_from_baseline(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91837220f45cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9513904ed828775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "648a1da321ede412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading pre-trained classifier: karhunen_nbestim_100_maxdepth_5.mod.json\n",
      "[INFO] Classifier: 100 trees, max_depth=5\n",
      "[INFO] Successfully loaded classifier with 100 trees\n"
     ]
    }
   ],
   "source": [
    "sklearn_rf, classifier_path = load_classifier_from_json(dataset_name, classifier_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594d99e8798eb761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Converting classifier to internal format...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"[INFO] Converting classifier to internal format...\")\n",
    "\n",
    "our_forest = sklearn_forest_to_forest(sklearn_rf, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf6d9f2d48e0eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_forest.trees[0].root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fee1556263f4a1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Storing forest and computing endpoints...\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 64 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 64 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "\n",
      "[INFO] Processing test samples...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['5.0',\n",
       " '6.0',\n",
       " '1.0',\n",
       " '3.0',\n",
       " '2.0',\n",
       " '1.0',\n",
       " '7.0',\n",
       " '9.0',\n",
       " '9.0',\n",
       " '8.0',\n",
       " '4.0',\n",
       " '9.0',\n",
       " '8.0',\n",
       " '8.0',\n",
       " '4.0',\n",
       " '6.0',\n",
       " '6.0',\n",
       " '5.0',\n",
       " '2.0',\n",
       " '9.0',\n",
       " '9.0',\n",
       " '5.0',\n",
       " '1.0',\n",
       " '6.0',\n",
       " '0.0',\n",
       " '5.0',\n",
       " '7.0',\n",
       " '0.0',\n",
       " '7.0',\n",
       " '0.0',\n",
       " '4.0',\n",
       " '2.0',\n",
       " '1.0',\n",
       " '7.0',\n",
       " '6.0',\n",
       " '5.0',\n",
       " '6.0',\n",
       " '0.0',\n",
       " '9.0',\n",
       " '1.0',\n",
       " '3.0',\n",
       " '3.0',\n",
       " '6.0',\n",
       " '7.0',\n",
       " '8.0',\n",
       " '2.0',\n",
       " '6.0',\n",
       " '7.0',\n",
       " '6.0',\n",
       " '4.0',\n",
       " '1.0',\n",
       " '6.0',\n",
       " '7.0',\n",
       " '8.0',\n",
       " '8.0',\n",
       " '5.0',\n",
       " '8.0',\n",
       " '8.0',\n",
       " '8.0',\n",
       " '4.0',\n",
       " '2.0',\n",
       " '7.0',\n",
       " '6.0',\n",
       " '3.0',\n",
       " '0.0',\n",
       " '2.0',\n",
       " '1.0',\n",
       " '0.0',\n",
       " '6.0',\n",
       " '6.0',\n",
       " '7.0',\n",
       " '2.0',\n",
       " '4.0',\n",
       " '1.0',\n",
       " '3.0',\n",
       " '7.0',\n",
       " '1.0',\n",
       " '5.0',\n",
       " '7.0',\n",
       " '8.0',\n",
       " '9.0',\n",
       " '1.0',\n",
       " '7.0',\n",
       " '8.0',\n",
       " '3.0',\n",
       " '7.0',\n",
       " '2.0',\n",
       " '1.0',\n",
       " '6.0',\n",
       " '6.0',\n",
       " '8.0',\n",
       " '0.0',\n",
       " '2.0',\n",
       " '3.0',\n",
       " '6.0',\n",
       " '3.0',\n",
       " '0.0',\n",
       " '7.0',\n",
       " '6.0',\n",
       " '0.0',\n",
       " '2.0',\n",
       " '1.0',\n",
       " '1.0',\n",
       " '9.0',\n",
       " '0.0',\n",
       " '4.0',\n",
       " '7.0',\n",
       " '5.0',\n",
       " '9.0',\n",
       " '3.0',\n",
       " '9.0',\n",
       " '2.0',\n",
       " '6.0',\n",
       " '2.0',\n",
       " '5.0',\n",
       " '8.0',\n",
       " '6.0',\n",
       " '4.0',\n",
       " '0.0',\n",
       " '3.0',\n",
       " '7.0',\n",
       " '9.0',\n",
       " '4.0',\n",
       " '7.0',\n",
       " '2.0',\n",
       " '9.0',\n",
       " '9.0',\n",
       " '7.0',\n",
       " '7.0',\n",
       " '0.0',\n",
       " '2.0',\n",
       " '1.0',\n",
       " '5.0',\n",
       " '9.0',\n",
       " '4.0',\n",
       " '9.0',\n",
       " '2.0',\n",
       " '4.0',\n",
       " '4.0',\n",
       " '9.0',\n",
       " '7.0',\n",
       " '3.0',\n",
       " '3.0',\n",
       " '2.0',\n",
       " '7.0',\n",
       " '3.0',\n",
       " '9.0',\n",
       " '7.0',\n",
       " '5.0',\n",
       " '1.0',\n",
       " '1.0',\n",
       " '6.0',\n",
       " '6.0',\n",
       " '5.0',\n",
       " '3.0',\n",
       " '1.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '3.0',\n",
       " '6.0',\n",
       " '1.0',\n",
       " '4.0',\n",
       " '3.0',\n",
       " '8.0',\n",
       " '5.0',\n",
       " '3.0',\n",
       " '1.0',\n",
       " '6.0',\n",
       " '9.0',\n",
       " '4.0',\n",
       " '6.0',\n",
       " '3.0',\n",
       " '4.0',\n",
       " '8.0',\n",
       " '0.0',\n",
       " '1.0',\n",
       " '2.0',\n",
       " '3.0',\n",
       " '1.0',\n",
       " '9.0',\n",
       " '3.0',\n",
       " '1.0',\n",
       " '1.0',\n",
       " '4.0',\n",
       " '9.0',\n",
       " '1.0',\n",
       " '8.0',\n",
       " '7.0',\n",
       " '7.0',\n",
       " '4.0',\n",
       " '5.0',\n",
       " '9.0',\n",
       " '3.0',\n",
       " '9.0',\n",
       " '1.0',\n",
       " '9.0',\n",
       " '0.0',\n",
       " '1.0',\n",
       " '5.0',\n",
       " '9.0']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\n[INFO] Storing forest and computing endpoints...\")\n",
    "eu_data = store_forest_and_endpoints(connections, our_forest)\n",
    "\n",
    "# 7. Process Test Samples\n",
    "print(\"\\n[INFO] Processing test samples...\")\n",
    "\n",
    "X_test = [dict(zip(feature_names,  x_test) ) for x_test in X_test_samples]\n",
    "\n",
    "# Since we don't have ground truth labels for samples,\n",
    "# we'll predict them and filter by target class\n",
    "\n",
    "predictions = [our_forest.predict(x_test) for x_test in X_test ]\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59c77c959469ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 samples not validated\n"
     ]
    }
   ],
   "source": [
    "from rcheck_cache import rcheck_cache, saturate\n",
    "\n",
    "validated= True\n",
    "not_validated = []\n",
    "validated_test = []\n",
    "for i in range(len(X_test)):\n",
    "    nodes = []\n",
    "    for tree in our_forest.trees:\n",
    "        nodes.append(tree.root)\n",
    "    caches = {\n",
    "        'R': set(),\n",
    "        'NR': set(),\n",
    "        'GP': set(),\n",
    "        'BP': set(),\n",
    "        'AR': set(),\n",
    "        'AP': set()\n",
    "    }\n",
    "    icf = our_forest.extract_icf(X_test[i])\n",
    "    if not rcheck_cache(\n",
    "                connections=connections,\n",
    "                icf=icf,\n",
    "                label=predictions[i],\n",
    "                nodes=saturate(icf, nodes),\n",
    "                eu_data=eu_data,\n",
    "                forest=our_forest,\n",
    "                caches=caches,\n",
    "                info={}\n",
    "            ):\n",
    "        not_validated.append(icf)\n",
    "        validated = False\n",
    "    else:\n",
    "        validated_test.append(X_test[i])\n",
    "\n",
    "if not validated:\n",
    "    print(f\"{len(not_validated)} samples not validated\")\n",
    "\n",
    "X_test = validated_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ffae1409e52db",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_validated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6618248239975605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned database DATA (DB 0)\n",
      "Cleaned database CAN (DB 1)\n",
      "Cleaned database R (DB 2)\n",
      "Cleaned database NR (DB 3)\n",
      "Cleaned database CAR (DB 4)\n",
      "Cleaned database AR (DB 5)\n",
      "Cleaned database GP (DB 6)\n",
      "Cleaned database BP (DB 7)\n",
      "Cleaned database PR (DB 8)\n",
      "Cleaned database AP (DB 9)\n",
      "Cleaned database LOGS (DB 10)\n",
      "\n",
      "Cleaned 11/11 databases\n"
     ]
    }
   ],
   "source": [
    "clean_all_databases(connections, db_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff75f0777c37b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = sorted(list(set(predictions)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "243f2265746fa9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Storing forest and computing endpoints...\n",
      "Storing Random Forest in DATA['RF']...\n",
      "Successfully stored forest with 100 trees in Redis key 'RF'\n",
      "[OK] Forest saved successfully\n",
      "Extracting feature thresholds...\n",
      "Extracted thresholds for 64 features\n",
      "Storing endpoints universe in DATA['EU']...\n",
      "Successfully stored dictionary with 64 keys in Redis key 'EU'\n",
      "[OK] Endpoints universe saved successfully\n",
      "[OK] Total test samples stored: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_samples = [sample for sample, pred in zip(X_test, predictions) if pred == label]\n",
    "\n",
    "print(\"\\n[INFO] Storing forest and computing endpoints...\")\n",
    "eu_data = store_forest_and_endpoints(connections, our_forest)\n",
    "\n",
    "store_dataset_total_samples(connections, len(label_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22bebe6a3c33c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icf_eu_encoding import bitmap_mask_to_string, icf_to_bitmap_mask\n",
    "from redis_helpers.samples import store_sample\n",
    "\n",
    "\n",
    "def process_all_classified_samples(\n",
    "    connections,\n",
    "    dataset_name,\n",
    "    class_label,\n",
    "    our_forest,\n",
    "    X_test,\n",
    "    eu_data,\n",
    "    dataset_type='generic',\n",
    "):\n",
    "    \"\"\"\n",
    "    Process all test samples that are classified with the specified class label.\n",
    "    Store samples in DATA and their ICF representations in R.\n",
    "\n",
    "    Args:\n",
    "        connections: Redis connections dict\n",
    "        dataset_name: Name of the dataset\n",
    "        class_label: Target class label to filter\n",
    "        our_forest: Custom Forest object\n",
    "        X_test: Test features array\n",
    "        y_test: Test labels array\n",
    "        feature_names: List of feature names\n",
    "        eu_data: Endpoints universe data\n",
    "        sample_percentage: Optional percentage of samples to process\n",
    "        dataset_type: Type of dataset ('uci', 'pmlb', 'openml', 'baseline', etc.)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (stored_samples list, summary dict)\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing All Samples Classified as '{class_label}' ===\")\n",
    "\n",
    "    # Find all test samples that are classified as the target class\n",
    "    target_samples_data = []\n",
    "    current_time = datetime.datetime.now().isoformat()\n",
    "\n",
    "    # Apply sample percentage filtering if specified\n",
    "\n",
    "\n",
    "    for i, sample in enumerate(X_test):\n",
    "        predicted_label = our_forest.predict(sample)\n",
    "\n",
    "        # Store ALL samples classified with the target label (regardless of correctness)\n",
    "        if predicted_label == class_label:\n",
    "            target_samples_data.append({\n",
    "                'test_index': i,\n",
    "                'sample_dict': sample,\n",
    "                'predicted_label': predicted_label,\n",
    "            })\n",
    "\n",
    "    print(f\"Found {len(target_samples_data)} samples classified as '{class_label}'\")\n",
    "\n",
    "    if len(target_samples_data) == 0:\n",
    "        print(\"[WARNING] No samples classified with the target label!\")\n",
    "        return [], {}\n",
    "\n",
    "    # Store all samples and their ICF representations\n",
    "    stored_samples = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for idx, sample_data in enumerate(target_samples_data):\n",
    "        sample_key = f\"sample_{dataset_name}_{class_label}_{idx}\"\n",
    "\n",
    "        # Store sample in DATA with full metadata\n",
    "        data_entry = {\n",
    "            'sample_dict': sample_data['sample_dict'],\n",
    "            'predicted_label': sample_data['predicted_label'],\n",
    "            'test_index': sample_data['test_index'],\n",
    "            'dataset_name': dataset_name,\n",
    "            'dataset_type': dataset_type,\n",
    "            'timestamp': current_time,\n",
    "        }\n",
    "\n",
    "        # Store sample using our helper function\n",
    "        if store_sample(connections['DATA'], sample_key, sample_data['sample_dict']):\n",
    "            # Also store full metadata separately\n",
    "            connections['DATA'].set(f\"{sample_key}_meta\", json.dumps(data_entry))\n",
    "\n",
    "        # Calculate ICF and store in R\n",
    "        try:\n",
    "            sample_icf = our_forest.extract_icf(sample_data['sample_dict'])\n",
    "            icf_bitmap = bitmap_mask_to_string(icf_to_bitmap_mask(sample_icf, eu_data))\n",
    "\n",
    "            # Store ICF bitmap in R with metadata\n",
    "            icf_metadata = {\n",
    "                'sample_key': sample_key,\n",
    "                'dataset_name': dataset_name,\n",
    "                'dataset_type': dataset_type,\n",
    "                'class_label': class_label,\n",
    "                'test_index': sample_data['test_index'],\n",
    "                'timestamp': current_time\n",
    "            }\n",
    "\n",
    "            connections['R'].set(icf_bitmap, json.dumps(icf_metadata))\n",
    "\n",
    "            stored_samples.append({\n",
    "                'sample_key': sample_key,\n",
    "                'icf_bitmap': icf_bitmap,\n",
    "                'test_index': sample_data['test_index']\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Failed to process sample {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Store summary information\n",
    "    summary = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'dataset_type': dataset_type,\n",
    "        'target_class_label': class_label,\n",
    "        'total_samples_processed': len(stored_samples),\n",
    "        'total_test_samples': len(X_test),\n",
    "        'samples_with_target_label': len(target_samples_data),\n",
    "        'timestamp': current_time,\n",
    "        'sample_keys': [s['sample_key'] for s in stored_samples]\n",
    "    }\n",
    "\n",
    "    connections['DATA'].set(f\"summary_{dataset_name}_{class_label}\", json.dumps(summary))\n",
    "\n",
    "    print(f\"[OK] Stored {len(stored_samples)} samples in DATA\")\n",
    "    print(f\"[OK] Stored {len(stored_samples)} ICF representations in R\")\n",
    "    print(f\"[OK] Summary stored in DATA['summary_{dataset_name}_{class_label}']\")\n",
    "\n",
    "    return stored_samples, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7278e869e3995a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing All Samples Classified as '0.0' ===\n",
      "Found 11 samples classified as '0.0'\n",
      "Successfully stored sample with 64 features in Redis key 'sample_karhunen_0.0_0'\n",
      "Successfully stored sample with 64 features in Redis key 'sample_karhunen_0.0_1'\n",
      "Successfully stored sample with 64 features in Redis key 'sample_karhunen_0.0_2'\n",
      "Successfully stored sample with 64 features in Redis key 'sample_karhunen_0.0_3'\n",
      "Successfully stored sample with 64 features in Redis key 'sample_karhunen_0.0_4'\n",
      "Successfully stored sample with 64 features in Redis key 'sample_karhunen_0.0_5'\n",
      "Successfully stored sample with 64 features in Redis key 'sample_karhunen_0.0_6'\n",
      "Successfully stored sample with 64 features in Redis key 'sample_karhunen_0.0_7'\n",
      "Successfully stored sample with 64 features in Redis key 'sample_karhunen_0.0_8'\n",
      "Successfully stored sample with 64 features in Redis key 'sample_karhunen_0.0_9'\n",
      "Successfully stored sample with 64 features in Redis key 'sample_karhunen_0.0_10'\n",
      "[OK] Stored 11 samples in DATA\n",
      "[OK] Stored 11 ICF representations in R\n",
      "[OK] Summary stored in DATA['summary_karhunen_0.0']\n"
     ]
    }
   ],
   "source": [
    "stored_samples, summary = process_all_classified_samples(\n",
    "            connections,\n",
    "            dataset_name,\n",
    "            label,\n",
    "            our_forest,\n",
    "            label_samples,\n",
    "            eu_data,\n",
    "            dataset_type='baseline'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b08c35e205b4877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Initializing seed candidates...\n",
      "Generating initial ICF and storing in CAN and PR...\n",
      "ICF calculated for 44 features\n",
      "Generated bitmap with 3096 bits\n",
      "[OK] Stored initial candidate in CAN\n",
      "[OK] Stored initial candidate in PR\n",
      "Generating initial ICF and storing in CAN and PR...\n",
      "ICF calculated for 45 features\n",
      "Generated bitmap with 3096 bits\n",
      "[OK] Stored initial candidate in CAN\n",
      "[OK] Stored initial candidate in PR\n",
      "Generating initial ICF and storing in CAN and PR...\n",
      "ICF calculated for 47 features\n",
      "Generated bitmap with 3096 bits\n",
      "[OK] Stored initial candidate in CAN\n",
      "[OK] Stored initial candidate in PR\n",
      "Generating initial ICF and storing in CAN and PR...\n",
      "ICF calculated for 42 features\n",
      "Generated bitmap with 3096 bits\n",
      "[OK] Stored initial candidate in CAN\n",
      "[OK] Stored initial candidate in PR\n",
      "Generating initial ICF and storing in CAN and PR...\n",
      "ICF calculated for 45 features\n",
      "Generated bitmap with 3096 bits\n",
      "[OK] Stored initial candidate in CAN\n",
      "[OK] Stored initial candidate in PR\n",
      "Generating initial ICF and storing in CAN and PR...\n",
      "ICF calculated for 45 features\n",
      "Generated bitmap with 3096 bits\n",
      "[OK] Stored initial candidate in CAN\n",
      "[OK] Stored initial candidate in PR\n",
      "Generating initial ICF and storing in CAN and PR...\n",
      "ICF calculated for 43 features\n",
      "Generated bitmap with 3096 bits\n",
      "[OK] Stored initial candidate in CAN\n",
      "[OK] Stored initial candidate in PR\n",
      "Generating initial ICF and storing in CAN and PR...\n",
      "ICF calculated for 47 features\n",
      "Generated bitmap with 3096 bits\n",
      "[OK] Stored initial candidate in CAN\n",
      "[OK] Stored initial candidate in PR\n",
      "Generating initial ICF and storing in CAN and PR...\n",
      "ICF calculated for 44 features\n",
      "Generated bitmap with 3096 bits\n",
      "[OK] Stored initial candidate in CAN\n",
      "[OK] Stored initial candidate in PR\n",
      "Generating initial ICF and storing in CAN and PR...\n",
      "ICF calculated for 49 features\n",
      "Generated bitmap with 3096 bits\n",
      "[OK] Stored initial candidate in CAN\n",
      "[OK] Stored initial candidate in PR\n",
      "Generating initial ICF and storing in CAN and PR...\n",
      "ICF calculated for 43 features\n",
      "Generated bitmap with 3096 bits\n",
      "[OK] Stored initial candidate in CAN\n",
      "[OK] Stored initial candidate in PR\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[INFO] Initializing seed candidates...\")\n",
    "for s in stored_samples:\n",
    "        meta_json = connections['DATA'].get(f\"{s['sample_key']}_meta\")\n",
    "        if meta_json:\n",
    "            meta = json.loads(meta_json)\n",
    "            initialize_seed_candidate(connections, meta, our_forest, eu_data)\n",
    "        else:\n",
    "            print(\"[WARNING] No samples processed, cannot initialize seed candidate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4842d8fa7c682128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Target label '0.0' set for worker processing\n",
      "\n",
      "[SUCCESS] Successfully initialized karhunen\n",
      "[SUCCESS] Pre-trained classifier loaded from: karhunen_nbestim_100_maxdepth_5.mod.json\n",
      "[SUCCESS] Ready for worker processing with 11 samples\n"
     ]
    }
   ],
   "source": [
    "# 9. Store target label for worker compatibility\n",
    "connections['DATA'].set('label', label)\n",
    "print(f\"[INFO] Target label '{label}' set for worker processing\")\n",
    "\n",
    "# 10. Store classifier metadata\n",
    "metadata = {\n",
    "            'dataset': dataset_name,\n",
    "            'classifier_path': classifier_path,\n",
    "            'n_estimators': sklearn_rf.n_estimators,\n",
    "            'max_depth': sklearn_rf.max_depth,\n",
    "            'n_features': sklearn_rf.n_features_in_,\n",
    "            'classes': list(sklearn_rf.classes_.astype(str)),\n",
    "            'timestamp': datetime.datetime.now().isoformat()\n",
    "}\n",
    "connections['DATA'].set('classifier_metadata', json.dumps(metadata))\n",
    "\n",
    "print(f\"\\n[SUCCESS] Successfully initialized {dataset_name}\")\n",
    "print(f\"[SUCCESS] Pre-trained classifier loaded from: {os.path.basename(classifier_path)}\")\n",
    "print(f\"[SUCCESS] Ready for worker processing with {len(stored_samples)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbdb6c6b9b1c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
